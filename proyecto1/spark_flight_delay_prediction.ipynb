{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark y ML: Predicción de retrasos aéreos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducción al Proyecto\n",
    "\n",
    "Este proyecto tiene como objetivo principal explorar y familiarizarse con el uso de Apache Spark como herramienta clave para el procesamiento masivo de datos, así como con el desarrollo y despliegue de proyectos en entornos de nube, específicamente utilizando Google Cloud Platform. Además, se aborda todo el proceso necesario para llevar a cabo un análisis de datos y construir un modelo de aprendizaje automático, siguiendo las buenas prácticas de la ciencia de datos, con el **objetivo** de predecir si un vuelo llegára con retraso o no.\n",
    "\n",
    "**Objetivos del Proyecto**:\n",
    "1. **Explorar Apache Spark**: Comprender y aplicar las funcionalidades de Spark para el manejo de grandes volúmenes de datos y el desarrollo de pipelines de machine learning.\n",
    "2. **Desarrollo en la Nube**: Utilizar entornos como Google Cloud para gestionar y procesar los datos, demostrando la escalabilidad de las herramientas cloud en proyectos reales.\n",
    "3. **Proceso de Ciencia de Datos**:\n",
    "   - Preprocesar y transformar los datos, incluyendo manejo de variables categóricas, normalización y ensamblaje de características.\n",
    "   - Construir y evaluar un modelo de clasificación binaria, concretamente un árbol de decisión, para predecir si un vuelo llegará con retraso o no.\n",
    "   - Analizar el rendimiento del modelo mediante herramientas como la matriz de confusión.\n",
    "\n",
    "Este proyecto integra tanto aspectos técnicos como analíticos, ofreciendo una experiencia práctica en el diseño y despliegue de proyectos que combinan Big Data, aprendizaje automático y computación en la nube.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leemos el fichero desde Google Cloud Storage \n",
    "\n",
    "- **Inicializa una sesión de Spark** en Dataproc.\n",
    "- **Carga un archivo CSV** (`flights.csv`) desde un bucket de Google Cloud Storage como un DataFrame de Spark.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5310bc8c3244ee98391957d26dc91d08",
     "grade": false,
     "grade_id": "lectura-fichero",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/lib/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-7d6be03e-6dd8-4afe-b73f-a1d4542cac0b;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.1.3 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.3 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.6.0 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.8-1 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.2 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.1.3/spark-sql-kafka-0-10_2.12-3.1.3.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.1.3!spark-sql-kafka-0-10_2.12.jar (26ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.1.3/spark-token-provider-kafka-0-10_2.12-3.1.3.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.3!spark-token-provider-kafka-0-10_2.12.jar (11ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/2.6.0/kafka-clients-2.6.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;2.6.0!kafka-clients.jar (57ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.6.2/commons-pool2-2.6.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-pool2;2.6.2!commons-pool2.jar (10ms)\n",
      "downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...\n",
      "\t[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (9ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/luben/zstd-jni/1.4.8-1/zstd-jni-1.4.8-1.jar ...\n",
      "\t[SUCCESSFUL ] com.github.luben#zstd-jni;1.4.8-1!zstd-jni.jar (48ms)\n",
      "downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.7.1/lz4-java-1.7.1.jar ...\n",
      "\t[SUCCESSFUL ] org.lz4#lz4-java;1.7.1!lz4-java.jar (12ms)\n",
      "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.8.2/snappy-java-1.1.8.2.jar ...\n",
      "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.8.2!snappy-java.jar(bundle) (21ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.30!slf4j-api.jar (10ms)\n",
      ":: resolution report :: resolve 3601ms :: artifacts dl 224ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.8-1 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.6.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.1.3 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.3 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   9   |   9   |   0   ||   9   |   9   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-7d6be03e-6dd8-4afe-b73f-a1d4542cac0b\n",
      "\tconfs: [default]\n",
      "\t9 artifacts copied, 0 already retrieved (13083kB/75ms)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/13 16:45:09 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "25/01/13 16:45:10 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "25/01/13 16:45:10 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/01/13 16:45:10 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "25/01/13 16:45:17 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.3.jar added multiple times to distributed cache.\n",
      "25/01/13 16:45:17 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.3.jar added multiple times to distributed cache.\n",
      "25/01/13 16:45:17 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-2.6.0.jar added multiple times to distributed cache.\n",
      "25/01/13 16:45:17 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar added multiple times to distributed cache.\n",
      "25/01/13 16:45:17 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar added multiple times to distributed cache.\n",
      "25/01/13 16:45:17 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.github.luben_zstd-jni-1.4.8-1.jar added multiple times to distributed cache.\n",
      "25/01/13 16:45:17 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar added multiple times to distributed cache.\n",
      "25/01/13 16:45:17 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.2.jar added multiple times to distributed cache.\n",
      "25/01/13 16:45:17 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar added multiple times to distributed cache.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Obtener la sesión de Spark preconfigurada en Dataproc\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Obtener la sesión de Spark preconfigurada en Dataproc\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Dataproc HDFS Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Cargado el archivo flights.csv desde google storage \n",
    "\n",
    "ruta_hdfs = \"gs://antonio-simon-bucket-1/notebooks/jupyter/flights.csv\" # Reemplaza esto por la ruta correcta del fichero flights.csv \n",
    "\n",
    "\n",
    "# Descomentar estas líneas\n",
    "flightsDF = spark.read\\\n",
    "             .option(\"header\", \"true\")\\\n",
    "             .option(\"inferSchema\", \"true\")\\\n",
    "             .csv(ruta_hdfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprimimos el esquema para comprobar qué tipo de dato ha inferido en cada columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- dep_time: string (nullable = true)\n",
      " |-- dep_delay: string (nullable = true)\n",
      " |-- arr_time: string (nullable = true)\n",
      " |-- arr_delay: string (nullable = true)\n",
      " |-- carrier: string (nullable = true)\n",
      " |-- tailnum: string (nullable = true)\n",
      " |-- flight: integer (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- air_time: string (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- minute: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightsDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostramos el número de filas que tiene el DataFrame para hacernos una idea de su tamaño:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "162049"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightsDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que tenemos 162049 filas. Si imprimimos por pantalla las 5 primeras filas, veremos qué tipos parecen tener y en qué columnas no coincide el tipo que podríamos esperar con el tipo que ha inferido Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|2014|    1|  1|       1|       96|     235|       70|     AS| N508AS|   145|   PDX| ANC|     194|    1542|   0|     1|\n",
      "|2014|    1|  1|       4|       -6|     738|      -23|     US| N195UW|  1830|   SEA| CLT|     252|    2279|   0|     4|\n",
      "|2014|    1|  1|       8|       13|     548|       -4|     UA| N37422|  1609|   PDX| IAH|     201|    1825|   0|     8|\n",
      "|2014|    1|  1|      28|       -2|     800|      -23|     US| N547UW|   466|   PDX| CLT|     251|    2282|   0|    28|\n",
      "|2014|    1|  1|      34|       44|     325|       43|     AS| N762AS|   121|   SEA| ANC|     201|    1448|   0|    34|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightsDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La causa del problema es que en muchas columnas existe un valor faltante llamado \"NA\". Spark no reconoce ese valor como\n",
    "*no disponible* ni nada similar, sino que lo considera como un string de texto normal, y por tanto, asigna a toda la columna\n",
    "el tipo de dato string (cadena de caracteres). Concretamente, las siguientes columnas deberían ser de tipo entero pero Spark\n",
    "las muestra como string:\n",
    "<ul>\n",
    " <li>dep_time: string (nullable = true)\n",
    " <li>dep_delay: string (nullable = true)\n",
    " <li>arr_time: string (nullable = true)\n",
    " <li>arr_delay: string (nullable = true)\n",
    " <li>air_time: string (nullable = true)\n",
    " <li>hour: string (nullable = true)\n",
    " <li>minute: string (nullable = true)    \n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a averiguar cuántas filas tienen el valor \"NA\" (como string) en la columna dep_time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "857"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "cuantos_NA = flightsDF\\\n",
    "                .where(F.col(\"dep_time\") == \"NA\")\\\n",
    "                .count()\n",
    "cuantos_NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por tanto, hay 857 filas que no tienen un dato válido en esa columna. Hay distintas maneras de trabajar con los valores faltantes, como por ejemplo imputarlos (reemplazarlos por un valor generado por nosotros según cierta lógica, por ejemplo la media de esa columna, etc). Lo más sencillo es quitar toda la fila, aunque esto depende de si nos lo podemos permitir en base\n",
    "a la cantidad de datos que tenemos. En nuestro caso, como tenemos un número considerable de filas, vamos a quitar todas las filas donde hay un NA en cualquiera de las columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[year: int, month: int, day: int, dep_time: string, dep_delay: string, arr_time: string, arr_delay: string, carrier: string, tailnum: string, flight: int, origin: string, dest: string, air_time: string, distance: int, hour: string, minute: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columnas_limpiar = [\"dep_time\", \"dep_delay\", \"arr_time\", \"arr_delay\", \"air_time\", \"hour\", \"minute\"]\n",
    "\n",
    "flightsLimpiado = flightsDF\n",
    "for nombreColumna in columnas_limpiar:  # para cada columna, nos quedamos con las filas que no tienen NA en esa columna\n",
    "    flightsLimpiado = flightsLimpiado.where(F.col(nombreColumna) != \"NA\")\n",
    "\n",
    "flightsLimpiado.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si ahora mostramos el número de filas que tiene el DataFrame `flightsLimpiado` tras eliminar todas esas filas, vemos que ha disminuido ligeramente\n",
    "pero sigue siendo un número considerable como para realizar analítica y sacar conclusiones sobre estos datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "160748"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightsLimpiado.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que hemos eliminado los NA, vamos a convertir a tipo entero cada una de esas columnas que eran de tipo string. \n",
    "Ahora no debe haber problema ya que todas las cadenas de texto contienen dentro un número que puede ser convertido de texto a número. Vamos también a convertir la columna `arr_delay` de tipo entero a número real, necesario para los pasos posteriores donde ajustaremos un modelo predictivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[year: int, month: int, day: int, dep_time: int, dep_delay: int, arr_time: int, arr_delay: double, carrier: string, tailnum: string, flight: int, origin: string, dest: string, air_time: int, distance: int, hour: int, minute: int]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "\n",
    "flightsConvertido = flightsLimpiado\n",
    "\n",
    "for c in columnas_limpiar:\n",
    "    # método que crea una columna o reemplaza una existente\n",
    "    flightsConvertido = flightsConvertido.withColumn(c, F.col(c).cast(IntegerType())) \n",
    "\n",
    "flightsConvertido = flightsConvertido.withColumn(\"arr_delay\", F.col(\"arr_delay\").cast(DoubleType()))\n",
    "flightsConvertido.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- dep_time: integer (nullable = true)\n",
      " |-- dep_delay: integer (nullable = true)\n",
      " |-- arr_time: integer (nullable = true)\n",
      " |-- arr_delay: double (nullable = true)\n",
      " |-- carrier: string (nullable = true)\n",
      " |-- tailnum: string (nullable = true)\n",
      " |-- flight: integer (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- air_time: integer (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- minute: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightsConvertido.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a volver a mostrar las 5 primeras filas del DataFrame limpio. Aparentemente son iguales a las que ya teníamos, pero ahora\n",
    "Spark sí está tratando como enteros las columnas que deberían serlo, y si queremos podemos hacer operaciones aritméticas\n",
    "con ellas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|2014|    1|  1|       1|       96|     235|     70.0|     AS| N508AS|   145|   PDX| ANC|     194|    1542|   0|     1|\n",
      "|2014|    1|  1|       4|       -6|     738|    -23.0|     US| N195UW|  1830|   SEA| CLT|     252|    2279|   0|     4|\n",
      "|2014|    1|  1|       8|       13|     548|     -4.0|     UA| N37422|  1609|   PDX| IAH|     201|    1825|   0|     8|\n",
      "|2014|    1|  1|      28|       -2|     800|    -23.0|     US| N547UW|   466|   PDX| CLT|     251|    2282|   0|    28|\n",
      "|2014|    1|  1|      34|       44|     325|     43.0|     AS| N762AS|   121|   SEA| ANC|     201|    1448|   0|    34|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "flightsConvertido.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de Aeropuertos y Rutas Únicas\n",
    "\n",
    "Para comprender mejor la estructura del conjunto de datos de vuelos almacenado en `flightsConvertido`realizamos un análisis de los aeropuertos de origen y las rutas disponibles. Los pasos seguidos son los siguientes:\n",
    "\n",
    "1. **Identificación de Aeropuertos de Origen Únicos**:\n",
    "   Es fundamental conocer cuántos aeropuertos distintos aparecen en los datos como puntos de origen. Para ello, se generará un DataFrame con los aeropuertos únicos presentes en la columna `origin` y, posteriormente, se calculará su cantidad total.\n",
    "\n",
    "2. **Determinación de Rutas Únicas**:\n",
    "   Comprender la diversidad de rutas aéreas es clave para analizar las conexiones disponibles. Para esto, se identifican las combinaciones únicas entre las columnas `origin` (aeropuerto de origen) y `dest` (aeropuerto de destino), lo que nos permitirá conocer cuántas rutas distintas existen.\n",
    "\n",
    "Estos pasos son fundamentales para explorar y estructurar mejor el análisis del tráfico aéreo representado en este conjunto de datos.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8fb2ec5d49ff84edae4833eca797068b",
     "grade": false,
     "grade_id": "ejercicio-1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de aeropuertos de origen únicos: 2\n",
      "Número de rutas únicas: 115\n"
     ]
    }
   ],
   "source": [
    "# Identificación de aeropuertos de origen únicos\n",
    "aeropuertosOrigenDF = flightsConvertido.select(\"origin\").distinct()\n",
    "\n",
    "# Cálculo de la cantidad total de aeropuertos de origen únicos\n",
    "n_origen = aeropuertosOrigenDF.count()\n",
    "\n",
    "# Identificación de rutas únicas entre origen y destino\n",
    "rutasDistintasDF = flightsConvertido.select(\"origin\", \"dest\").distinct()\n",
    "\n",
    "# Cálculo del número total de rutas únicas\n",
    "n_rutas = rutasDistintasDF.count()\n",
    "\n",
    "# Salida de los resultados:\n",
    "print(f\"Número de aeropuertos de origen únicos: {n_origen}\")\n",
    "print(f\"Número de rutas únicas: {n_rutas}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de Retrasos Medios en Aeropuertos de Destino\n",
    "\n",
    "Para analizar los patrones de retraso en los vuelos, se planteó calcular el retraso medio de llegada en cada aeropuerto de destino, considerando únicamente los vuelos que llegan con un retraso positivo. Este análisis puede ayudar a identificar qué aeropuertos tienen, en promedio, mayores problemas con la puntualidad.\n",
    "\n",
    "Los pasos seguidos para realizar este análisis son los siguientes:\n",
    "\n",
    "1. **Filtrar los vuelos con retraso positivo**:\n",
    "   Se consideraron únicamente los vuelos que tienen valores positivos en la columna `arr_delay`, descartando aquellos que llegaron puntuales o adelantados.\n",
    "\n",
    "2. **Calcular el retraso medio por aeropuerto de destino**:\n",
    "   Agrupando los datos por la columna `dest` (aeropuerto de destino), se calculó el promedio de los retrasos en la columna `arr_delay`. La nueva columna, que contiene el retraso medio, se denominó `retraso_medio`.\n",
    "\n",
    "3. **Ordenar los resultados**:\n",
    "   Para facilitar la interpretación, los resultados se ordenaron de mayor a menor según el valor de `retraso_medio`.\n",
    "\n",
    "4. **Encapsular el cálculo en una función**:\n",
    "   Con el objetivo de que el análisis sea reutilizable y más claro, todo el proceso fue encapsulado en una función llamada `retrasoMedio`. Esta función toma como entrada un DataFrame de Spark y devuelve un nuevo DataFrame con los cálculos realizados.\n",
    "\n",
    "Este enfoque no solo permite identificar los aeropuertos con mayores problemas de retraso, sino que también facilita la integración de esta funcionalidad en futuros análisis.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|dest|     retraso_medio|\n",
      "+----+------------------+\n",
      "| BOI|             64.75|\n",
      "| HDN|              46.8|\n",
      "| SFO|41.193768844221104|\n",
      "| CLE| 35.74193548387097|\n",
      "| SBA|35.391752577319586|\n",
      "| COS| 35.05607476635514|\n",
      "| BWI|34.585798816568044|\n",
      "| EWR| 33.52972258916777|\n",
      "| DFW| 33.27519181585678|\n",
      "| MIA| 32.66187050359712|\n",
      "| ORD| 32.47909024211299|\n",
      "| BNA| 31.94871794871795|\n",
      "| JFK|31.255884586180713|\n",
      "| JAC|             30.25|\n",
      "| PHL|29.245989304812834|\n",
      "| OGG|27.511111111111113|\n",
      "| IAD|27.430875576036865|\n",
      "| HOU| 27.33009708737864|\n",
      "| LGB| 27.07634730538922|\n",
      "| FAT|26.852589641434264|\n",
      "+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Definir la función para calcular el retraso medio\n",
    "def retrasoMedio(df):\n",
    "    \"\"\"\n",
    "    Calcula el retraso medio de llegada para cada aeropuerto de destino,\n",
    "    considerando únicamente los vuelos con retraso positivo.\n",
    "    \n",
    "    :param df: DataFrame de Spark con información de vuelos.\n",
    "    :return: DataFrame con el retraso medio por aeropuerto de destino, ordenado de mayor a menor.\n",
    "    \"\"\"\n",
    "    # Filtrar los vuelos con retraso positivo\n",
    "    df_retraso_positivo = df.filter(F.col(\"arr_delay\") > 0)\n",
    "    \n",
    "    # Calcular el retraso medio agrupado por destino\n",
    "    df_resultado = df_retraso_positivo.groupBy(\"dest\") \\\n",
    "        .agg(F.avg(\"arr_delay\").alias(\"retraso_medio\")) \\\n",
    "        .orderBy(F.col(\"retraso_medio\").desc())\n",
    "    \n",
    "    return df_resultado\n",
    "\n",
    "# Aplicar la función al DataFrame flightsConvertido\n",
    "retrasoMedioDF = retrasoMedio(flightsConvertido)\n",
    "\n",
    "# Se muestran los resultados\n",
    "retrasoMedioDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora invocamos a nuestra función `retrasoMedio` pasándole como argumento `flightsConvertido`, y se muestran los tres aeropuertos con mayor retraso en minutos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aeropuerto: BOI, Retraso medio: 64.75 minutos\n",
      "Aeropuerto: HDN, Retraso medio: 46.80 minutos\n",
      "Aeropuerto: SFO, Retraso medio: 41.19 minutos\n"
     ]
    }
   ],
   "source": [
    "# Obtenemos los tres primeros aeropuertos con mayor retraso medio\n",
    "top3 = retrasoMedioDF.take(3)\n",
    "\n",
    "# Mostramos ahora el resultado de los tres aeropuertos y sus retrasos en minutos\n",
    "for fila in top3:\n",
    "    print(f\"Aeropuerto: {fila['dest']}, Retraso medio: {fila['retraso_medio']:.2f} minutos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo de Clasificación para Predecir Retrasos\n",
    "\n",
    "El objetivo es ajustar un modelo de clasificación binaria utilizando un árbol de decisión (`DecisionTreeClassifier`) en Spark para predecir si un vuelo llegará con retraso. Como variables predictoras se utilizan:\n",
    "\n",
    "- El mes (`month`).\n",
    "- El día del mes (`day`).\n",
    "- La hora de partida (`dep_time`).\n",
    "- La hora de llegada (`arr_time`).\n",
    "- El tipo de avión (`carrier`).\n",
    "- La distancia (`distance`).\n",
    "- El tiempo en el aire (`air_time`).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pyspark.ml.feature import StringIndexer \n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import Binarizer\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml import PipelineModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparación de Variables Categóricas para el Modelo de Clasificación\n",
    "\n",
    "En el conjunto de datos que se utilizará para ajustar un modelo de clasificación binaria, es importante distinguir entre variables numéricas y categóricas. Aunque algunas variables, como el mes (`month`) y el tipo de avión (`carrier`), están representadas como números, en realidad son variables categóricas que representan categorías discretas. Para que Spark pueda procesarlas correctamente en un modelo de aprendizaje automático, deben ser indexadas.\n",
    "\n",
    "#### Conversión de Variables Categóricas\n",
    "1. **Motivación**:\n",
    "   - Las variables categóricas deben transformarse en índices numéricos para que los modelos de Spark las interpreten correctamente.\n",
    "   - Esto se hace utilizando la clase `StringIndexer` de la librería `pyspark.ml.feature`.\n",
    "\n",
    "2. **Definición del Proceso**:\n",
    "   - Se crea un `StringIndexer` para la columna `month`, generando una nueva columna llamada `monthIndexed` con los valores indexados.\n",
    "   - De forma similar, se crea otro `StringIndexer` para la columna `carrier`, generando la columna `carrierIndexed`.\n",
    "\n",
    "Estas transformaciones son fundamentales para garantizar que el modelo pueda interpretar correctamente las variables categóricas durante el proceso de entrenamiento.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb039584cd14e6bc3434e5be930341e6",
     "grade": false,
     "grade_id": "string-indexer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Crear un StringIndexer para la columna 'month'\n",
    "indexerMonth = StringIndexer(inputCol=\"month\", outputCol=\"monthIndexed\")\n",
    "\n",
    "# Crear un StringIndexer para la columna 'carrier'\n",
    "indexerCarrier = StringIndexer(inputCol=\"carrier\", outputCol=\"carrierIndexed\")\n",
    "\n",
    "# Indagar mas en los indexed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de la Columna de Características para el Modelo\n",
    "\n",
    "Para entrenar un modelo de clasificación en Spark, es necesario que todas las variables predictoras estén combinadas en una única columna de tipo vector. Este formato es requerido por los algoritmos de aprendizaje automático de Spark. \n",
    "\n",
    "#### Proceso de Ensamblado de Características\n",
    "1. **Motivación**:\n",
    "   - Las variables predictoras deben fusionarse en una única representación vectorial para facilitar su uso por el modelo.\n",
    "   - Este paso también asegura que las columnas categóricas indexadas (`monthIndexed` y `carrierIndexed`) y las numéricas originales se combinen correctamente.\n",
    "\n",
    "2. **Definición del Proceso**:\n",
    "   - Se crea una lista con los nombres de las columnas que se utilizarán como características predictoras, excluyendo la variable objetivo `arr_delay`.\n",
    "   - En esta lista se incluyen las columnas indexadas (`monthIndexed` y `carrierIndexed`) en lugar de las columnas originales (`month` y `carrier`).\n",
    "   - Con la lista de columnas definida, se utiliza la clase `VectorAssembler` para ensamblar las características en una nueva columna llamada `features`.\n",
    "\n",
    "Este paso garantiza que las variables estén listas para ser procesadas por el modelo, respetando el formato requerido por Spark.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "44623471d64d0605fcbcd4bbcc3c2a0d",
     "grade": false,
     "grade_id": "vector-assembler",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Definir las columnas a ensamblar como características predictoras\n",
    "columnas_ensamblar = [\"monthIndexed\", \"day\", \"dep_time\", \"arr_time\", \"distance\", \"air_time\", \"carrierIndexed\"]\n",
    "\n",
    "# Crear el VectorAssembler para generar la columna 'features'\n",
    "vectorAssembler = VectorAssembler(inputCols=columnas_ensamblar, outputCol=\"features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversión de la Variable Objetivo a un Formato Binario\n",
    "\n",
    "En este análisis, la columna `arr_delay`, que representa el retraso en minutos de cada vuelo, es una variable continua. Sin embargo, para abordar el problema como una clasificación binaria, es necesario convertir esta columna en una variable binaria que indique si un vuelo es considerado retrasado o no.\n",
    "\n",
    "#### Proceso de Binarización\n",
    "1. **Motivación**:\n",
    "   - Un vuelo se considerará **retrasado** si tiene más de 15 minutos de retraso en la llegada (`arr_delay > 15`).\n",
    "   - Caso contrario, será clasificado como **no retrasado**.\n",
    "   - Este enfoque permite que la columna `arr_delay_binary` sea utilizada como la variable objetivo (`target`) en el modelo de clasificación.\n",
    "\n",
    "2. **Definición del Proceso**:\n",
    "   - Se utiliza la clase `Binarizer` de Spark para transformar la columna continua `arr_delay` en la columna binaria `arr_delay_binary`.\n",
    "   - El umbral (`threshold`) se establece en 15 minutos.\n",
    "   - Esta nueva columna será interpretada como la variable objetivo del modelo, y por ello no se incluyó en las características (`features`) ensambladas previamente.\n",
    "\n",
    "Este paso asegura que la variable objetivo sea adecuada para el problema de clasificación binaria, alineándose con el objetivo del análisis.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1f33c493a3c3dd83fb6a39a7acefca5c",
     "grade": false,
     "grade_id": "binarizer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Crear el Binarizer con umbral de 15 minutos\n",
    "delayBinarizer = Binarizer(inputCol=\"arr_delay\", outputCol=\"arr_delay_binary\", threshold=15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuración del Modelo de Clasificación\n",
    "\n",
    "Una vez preparadas las características predictoras y la variable objetivo, el siguiente paso es definir el modelo de clasificación que se utilizará para predecir si un vuelo llegará con retraso o no. Para ello, se selecciona un árbol de decisión, un algoritmo interpretable y eficiente para problemas de clasificación.\n",
    "\n",
    "#### Proceso de Configuración del Modelo\n",
    "1. **Selección del Modelo**:\n",
    "   - Se utiliza el `DecisionTreeClassifier` de la librería `pyspark.ml.classification`, que implementa árboles de decisión para clasificación binaria.\n",
    "\n",
    "2. **Definición de las Columnas**:\n",
    "   - La columna de entrada (`featuresCol`) será `features`, creada previamente con el `VectorAssembler` y que contiene todas las características predictoras combinadas.\n",
    "   - La columna objetivo (`labelCol`) será `arr_delay_binary`, obtenida tras binarizar la variable continua `arr_delay`.\n",
    "\n",
    "3. **Motivación**:\n",
    "   - Los árboles de decisión son ideales para explorar patrones en los datos y entender las decisiones que toma el modelo debido a su naturaleza interpretativa.\n",
    "   - Este enfoque permite abordar el problema de clasificación de forma estructurada y utilizando herramientas que maximizan la utilidad del conjunto de datos preparado.\n",
    "\n",
    "Con esta configuración, el modelo queda listo para ser entrenado utilizando los datos procesados.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e785136d6d06691c003ff9542027e03d",
     "grade": false,
     "grade_id": "decision-tree",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Configurar el modelo de clasificación\n",
    "decisionTree = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"arr_delay_binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación y Entrenamiento de un Pipeline de Clasificación\n",
    "\n",
    "Para simplificar y automatizar el proceso completo de preprocesamiento, modelado y predicción, se decidió encapsular todas las etapas previas en un único pipeline de Spark. Esto permite realizar todas las transformaciones y entrenar el modelo de forma estructurada y reproducible.\n",
    "\n",
    "#### Proceso de Creación y Entrenamiento del Pipeline\n",
    "1. **Motivación**:\n",
    "   - Un pipeline permite definir de manera clara y ordenada todas las etapas del flujo de trabajo, desde el preprocesamiento de datos hasta la generación del modelo.\n",
    "   - Este enfoque asegura que cada etapa se aplique en el orden correcto, reduciendo el riesgo de errores y facilitando la reutilización del modelo.\n",
    "\n",
    "2. **Definición del Pipeline**:\n",
    "   - Las etapas del pipeline incluyen:\n",
    "     - **Indexación de Variables Categóricas**: Uso de `indexerMonth` y `indexerCarrier` para convertir variables categóricas en índices numéricos.\n",
    "     - **Ensamblaje de Características**: Uso de `vectorAssembler` para combinar las columnas predictoras en una única columna de tipo vector.\n",
    "     - **Binarización de la Variable Objetivo**: Uso de `delayBinarizer` para convertir la variable continua `arr_delay` en una columna binaria `arr_delay_binary`.\n",
    "     - **Modelo de Clasificación**: Uso de un árbol de decisión `decisionTree` para predecir si un vuelo tendrá retraso o no.\n",
    "\n",
    "3. **Entrenamiento del Pipeline**:\n",
    "   - El pipeline es entrenado con los datos disponibles (`flightsConvertido`), generando un modelo entrenado que combina todas las etapas.\n",
    "\n",
    "4. **Predicción sobre los Datos de Entrenamiento**:\n",
    "   - Por simplicidad, se aplica el pipeline entrenado al mismo conjunto de datos utilizado para el entrenamiento, generando predicciones que se almacenan en un nuevo DataFrame.\n",
    "\n",
    "Este pipeline representa un flujo de trabajo automatizado y eficiente que facilita el preprocesamiento y el modelado en Spark.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+----------+\n",
      "|            features|arr_delay_binary|prediction|\n",
      "+--------------------+----------------+----------+\n",
      "|[10.0,1.0,1.0,235...|             1.0|       1.0|\n",
      "|[10.0,1.0,4.0,738...|             0.0|       0.0|\n",
      "|[10.0,1.0,8.0,548...|             0.0|       0.0|\n",
      "|[10.0,1.0,28.0,80...|             0.0|       0.0|\n",
      "|[10.0,1.0,34.0,32...|             1.0|       1.0|\n",
      "|[10.0,1.0,37.0,74...|             1.0|       0.0|\n",
      "|[10.0,1.0,346.0,9...|             1.0|       0.0|\n",
      "|[10.0,1.0,526.0,1...|             0.0|       0.0|\n",
      "|[10.0,1.0,527.0,9...|             1.0|       0.0|\n",
      "|[10.0,1.0,536.0,1...|             0.0|       0.0|\n",
      "+--------------------+----------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Crear el pipeline con las etapas definidas previamente\n",
    "pipeline = Pipeline(stages=[indexerMonth, indexerCarrier, vectorAssembler, delayBinarizer, decisionTree])\n",
    "\n",
    "# Entrenar el pipeline con los datos disponibles\n",
    "pipelineModel = pipeline.fit(flightsConvertido)\n",
    "\n",
    "# Aplicar el pipeline entrenado para realizar predicciones\n",
    "flightsPredictions = pipelineModel.transform(flightsConvertido)\n",
    "\n",
    "# Verificar las predicciones \n",
    "flightsPredictions.select(\"features\", \"arr_delay_binary\", \"prediction\").show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación del Modelo: Matriz de Confusión\n",
    "\n",
    "Para evaluar el rendimiento del modelo de clasificación, se decidió construir una matriz de confusión. Esta matriz permite comparar las predicciones generadas por el modelo con las clases verdaderas, lo que facilita identificar los aciertos y los errores cometidos.\n",
    "\n",
    "#### Proceso de Construcción de la Matriz de Confusión\n",
    "1. **Agrupación por Clases Verdaderas y Predichas**:\n",
    "   - Se agrupan los datos por las columnas `arr_delay_binary` (clase verdadera) y `prediction` (clase predicha).\n",
    "   - Para cada combinación, se calcula el número de casos.\n",
    "\n",
    "2. **Interpretación**:\n",
    "   - Cada fila de la matriz representa una combinación de la clase verdadera y la predicha.\n",
    "   - Los casos en que la clase verdadera coincide con la predicha se consideran aciertos.\n",
    "   - Las discrepancias reflejan los errores del modelo.\n",
    "\n",
    "Esta matriz es una herramienta clave para analizar el desempeño del modelo y determinar posibles áreas de mejora.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "75f98ae39b827e75c3f0b4b2aaa6b0db",
     "grade": false,
     "grade_id": "cell-896752beb71cb455",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 63:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+------+\n",
      "|arr_delay_binary|prediction| count|\n",
      "+----------------+----------+------+\n",
      "|             1.0|       1.0|  1646|\n",
      "|             0.0|       1.0|   919|\n",
      "|             1.0|       0.0| 22603|\n",
      "|             0.0|       0.0|135580|\n",
      "+----------------+----------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "flightsPredictions.groupBy(\"arr_delay_binary\", \"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusión\n",
    "\n",
    "Este proyecto ha permitido desarrollar y evaluar un modelo de clasificación binaria para predecir si un vuelo llegará con retraso o no, utilizando un pipeline de Spark que integra todo el flujo de preprocesamiento y modelado. \n",
    "\n",
    "**Principales Resultados**:\n",
    "- **Precisión Global**: El modelo tiene un rendimiento aceptable, con una precisión global del 85.35%.\n",
    "- **Fortalezas**:\n",
    "  - Identifica con alta precisión los vuelos que no llegan con retraso (99.32% de precisión en esta categoría).\n",
    "- **Debilidades**:\n",
    "  - Presenta dificultades para detectar vuelos retrasados, con una cobertura baja (6.78%) en esta clase.\n",
    "\n",
    "**Áreas de Mejora**:\n",
    "1. Balancear el conjunto de datos para reducir el sesgo hacia la clase mayoritaria (vuelos no retrasados).\n",
    "2. Explorar algoritmos más avanzados, como Random Forest o Gradient Boosted Trees, para mejorar la capacidad predictiva.\n",
    "3. Considerar la inclusión de nuevas variables predictoras o transformaciones que aporten más información al modelo.\n",
    "\n",
    "En general, el proyecto proporciona una base sólida para analizar patrones de retraso en vuelos y explorar mejoras adicionales en futuras iteraciones.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
